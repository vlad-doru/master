### READING
# Read data from the .csv file.
data <- read.csv("./input/sentiment.csv")
# Remove neutral tweets from the dataset and keep only sentiment and text columns
tweets <- data[data["sentiment"] != "Neutral",c("sentiment", "text")]
tweets[,"sentiment"] <- factor(tweets[,"sentiment"])
# Log how many tweets we have loaded.
sprintf("Number of available tweets is %d.", nrow(tweets))
### PREPROCESSING
library(tm)
library(RTextTools)
library(SnowballC)
library(RWeka)
# Make all words lowercase in all of the tweets.
tweets[,"text"] <- sapply(tweets[,"text"], tolower)
# Remove all the URLs as they are not relevant.
tweets[, "text"] <- sapply(tweets[,"text"], function(x) gsub("http\\S+\\s*", "", x))
# Remove all the @ mentions.
tweets[, "text"] <- sapply(tweets[,"text"], function(x) gsub("@\\S+\\s*", "", x))
# Remove the # character so that we have better matching.
tweets[, "text"] <- sapply(tweets[,"text"], function(x) gsub("#", "", x))
# Remove the " character.
tweets[, "text"] <- sapply(tweets[,"text"], function(x) gsub("\"", "", x))
# Construct the corpus of our tweets.
tweets_corpus <- Corpus(VectorSource(tweets[,"text"]))
# Construct the document term matrix.
dt_matrix <- DocumentTermMatrix(tweets_corpus,
control=list(
wordLengths = c(1, Inf),
bounds = list(global = c(5, Inf)),  # at least 5 documents
weighting = weightTfIdf,            # weight by tf-id
removePunctuation = TRUE,
stemming = TRUE
))
features_matrix <- as.matrix(dt_matrix)
### ML Part
library(caret)
library(dplyr)         # Used by caret
library(kernlab)       # support vector machine
library(pROC)
trainIndex <- createDataPartition(tweets$sentiment,p=.10,list=FALSE)
trainData <- features_matrix[trainIndex,]
testData <- features_matrix[-trainIndex,]
trainLabels <- tweets[trainIndex, "sentiment"]
testLabels <- tweets[-trainIndex, "sentiment"]
#Train and Tune the SVM
svm.tune <- train(x=trainData,
y= trainLabels,
method = "nb")
### READING
# Read data from the .csv file.
data <- read.csv("./input/sentiment.csv")
# Remove neutral tweets from the dataset and keep only sentiment and text columns
tweets <- data[data["sentiment"] != "Neutral",c("sentiment", "text")]
tweets[,"sentiment"] <- factor(tweets[,"sentiment"])
# Log how many tweets we have loaded.
sprintf("Number of available tweets is %d.", nrow(tweets))
### PREPROCESSING
library(tm)
library(RTextTools)
library(SnowballC)
library(RWeka)
# Make all words lowercase in all of the tweets.
tweets[,"text"] <- sapply(tweets[,"text"], tolower)
# Remove all the URLs as they are not relevant.
tweets[, "text"] <- sapply(tweets[,"text"], function(x) gsub("http\\S+\\s*", "", x))
# Remove all the @ mentions.
tweets[, "text"] <- sapply(tweets[,"text"], function(x) gsub("@\\S+\\s*", "", x))
# Remove the # character so that we have better matching.
tweets[, "text"] <- sapply(tweets[,"text"], function(x) gsub("#", "", x))
# Remove the " character.
tweets[, "text"] <- sapply(tweets[,"text"], function(x) gsub("\"", "", x))
# Construct the corpus of our tweets.
tweets_corpus <- Corpus(VectorSource(tweets[,"text"]))
# Construct the document term matrix.
dt_matrix <- DocumentTermMatrix(tweets_corpus,
control=list(
wordLengths = c(1, Inf),
bounds = list(global = c(10, Inf)),  # at least 5 documents
weighting = weightBin
#weighting = weightTfIdf            # weight by tf-id
))
features_matrix <- as.matrix(dt_matrix)
print(dim(features_matrix))
### ML Part
library(caret)
library(dplyr)         # Used by caret
library(kernlab)       # support vector machine
library(pROC)
library(vbmp)
features_matrix = features_matrix[, 1:5]
print(dim(features_matrix))
trainIndex <- createDataPartition(tweets$sentiment,p=.01,list=FALSE)
trainData <- features_matrix[trainIndex,]
testData <- features_matrix[-trainIndex,]
trainLabels <- tweets[trainIndex, "sentiment"]
testLabels <- tweets[-trainIndex, "sentiment"]
print(trainData)
naive <- train(trainData, trainLabels, method="vbmpRadial", trControl=trainControl(method='cv',number=10), verbose = TRUE)
warnings()
naive <- train(trainData, trainLabels, method="nb", trControl=trainControl(method='cv',number=10), verbose = TRUE)
warnings()
testData <- testData[1:100, ]
p <- predict(naive, testData)
print(p)
features_matrix <- as.matrix(dt_matrix)
print(dim(features_matrix))
trainIndex <- createDataPartition(tweets$sentiment,p=.01,list=FALSE)
trainData <- features_matrix[trainIndex,]
testData <- features_matrix[-trainIndex,]
trainLabels <- tweets[trainIndex, "sentiment"]
testLabels <- tweets[-trainIndex, "sentiment"]
print(trainData)
dim(trainData)
naive <- train(trainData, trainLabels, method="nb", trControl=trainControl(method='cv',number=10), verbose = TRUE)
naive <- train(trainData, trainLabels, method="nb", trControl=trainControl(verbose = TRUE, method='cv',number=10), verbose = TRUE)
testData <- testData[1:100, ]
p <- predict(naive, testData)
summary(p)
testData <- features_matrix[-trainIndex,]
testData <- testData[1:1000, ]
length(testData)
testData <- features_matrix[-trainIndex,]
length(testData)
testData <- features_matrix[-trainIndex,]
length(testData)
print(dim(features_matrix))
dim(testData)
testData <- testData[1:1000, ]
dim(testData)
p <- predict(naive, testData)
?predict
p <- predict(naive, testData, verbose = TRUE)
naive
summary(trainLabels)
naive
trainIndex <- createDataPartition(tweets$sentiment,p=.10,list=FALSE)
trainData <- features_matrix[trainIndex,]
testData <- features_matrix[-trainIndex,]
trainLabels <- tweets[trainIndex, "sentiment"]
testLabels <- tweets[-trainIndex, "sentiment"]
dim(trainData)
naive <- train(trainData, trainLabels, method="nb", trControl=trainControl(verbose = TRUE, method='cv',number=10), verbose = TRUE)
summary(trainData)
max(trainData)
summary(trainData)
summary(trainData)$Mean
colMeans(trainData)
max(colMeans(trainData))
max(colMeans(trainData))
sort(colMeans(trainData))
trainIndex <- createDataPartition(tweets$sentiment,p=.90,list=FALSE)
trainData <- features_matrix[trainIndex,]
testData <- features_matrix[-trainIndex,]
trainLabels <- tweets[trainIndex, "sentiment"]
testLabels <- tweets[-trainIndex, "sentiment"]
naive <- train(trainData, trainLabels, method="nb", trControl=trainControl(verbose = TRUE, method='cv',number=1), verbose = TRUE)
naive <- train(trainData, trainLabels, method="nb", trControl=trainControl(verbose = TRUE, method='none'), verbose = TRUE)
?trainControl
naive <- train(trainData, trainLabels, method="nb", trControl=trainControl(verbose = TRUE, method='none'), verbose = TRUE)
naive <- train(trainData, trainLabels, method="nb", trControl=trainControl(verbose = TRUE, method='none'),)
naive <- train(trainData, trainLabels, method="nb", tuneGrid = NULL, trControl=trainControl(verbose = TRUE, method='none'),)
naive <- train(trainData, trainLabels, method="nb", trControl=trainControl(verbose = TRUE, method='none'),)
naive <- train(trainData, trainLabels, method="nb", trControl=trainControl(verbose = TRUE, method='none'))
naive <- train(trainData, trainLabels, method="nb", tuneGrid=data.frame(mtry=3), trControl=trainControl(verbose = TRUE, method='none'))
naive <- train(trainData, trainLabels, method="nb", tuneGrid=data.frame(fL = 0, adjust = 0, userkernel = FALSE), trControl=trainControl(verbose = TRUE, method='none'))
naive <- train(trainData, trainLabels, method="nb", tuneGrid=data.frame(fL = 0, adjust = 0, usekernel = FALSE), trControl=trainControl(verbose = TRUE, method='none'))
dt_matrix <- DocumentTermMatrix(tweets_corpus,
control=list(
wordLengths = c(1, Inf),
bounds = list(global = c(20, Inf)),  # at least 5 documents
weighting = weightBin
#weighting = weightTfIdf            # weight by tf-id
))
features_matrix <- as.matrix(dt_matrix)
print(dim(features_matrix))
trainIndex <- createDataPartition(tweets$sentiment,p=.90,list=FALSE)
trainData <- features_matrix[trainIndex,]
testData <- features_matrix[-trainIndex,]
trainLabels <- tweets[trainIndex, "sentiment"]
testLabels <- tweets[-trainIndex, "sentiment"]
naive <- train(trainData, trainLabels, method="nb", tuneGrid=data.frame(fL = 0, adjust = 0, usekernel = FALSE), trControl=trainControl(verbose = TRUE, method='none'))
print(colSums(features_matrix))
print(min(colSums(features_matrix)))
bounds = list(global = c(50, Inf)),  # at least 5 documents
dt_matrix <- DocumentTermMatrix(tweets_corpus,
control=list(
wordLengths = c(1, Inf),
bounds = list(global = c(50, Inf)),  # at least 5 documents
weighting = weightBin
#weighting = weightTfIdf            # weight by tf-id
))
features_matrix <- as.matrix(dt_matrix)
print(dim(features_matrix))
dt_matrix <- DocumentTermMatrix(tweets_corpus,
control=list(
wordLengths = c(1, Inf),
bounds = list(global = c(20, Inf)),  # at least 5 documents
weighting = weightBin
#weighting = weightTfIdf            # weight by tf-id
))
features_matrix <- as.matrix(dt_matrix)
print(dim(features_matrix))
trainIndex <- createDataPartition(tweets$sentiment,p=.90,list=FALSE)
trainData <- features_matrix[trainIndex,]
testData <- features_matrix[-trainIndex,]
trainLabels <- tweets[trainIndex, "sentiment"]
testLabels <- tweets[-trainIndex, "sentiment"]
naive <- train(trainData, trainLabels, method="nb", tuneGrid=data.frame(fL = 0, adjust = 0, usekernel = FALSE), trControl=trainControl(verbose = TRUE, method='none'))
trainIndex <- createDataPartition(tweets$sentiment,p=.90,list=FALSE)
trainData <- features_matrix[trainIndex,]
zv <- nearZeroVar(trainData, saveMetrics = TRUE)$zeroVar
trainData <- features_matrix[trainIndex, -zv]
testData <- features_matrix[-trainIndex, -zv]
trainLabels <- tweets[trainIndex, "sentiment"]
testLabels <- tweets[-trainIndex, "sentiment"]
naive <- train(trainData, trainLabels, method="nb", tuneGrid=data.frame(fL = 0, adjust = 0, usekernel = FALSE), trControl=trainControl(verbose = TRUE, method='none'))
print(naive)
naive <- train(trainData, trainLabels, method="nb", tuneGrid=data.frame(fL = 0, adjust = 0, usekernel = TRUE), trControl=trainControl(verbose = TRUE, method='none'))
naive <- train(trainData, trainLabels, method="nb", tuneGrid=data.frame(fL = 0, adjust = 0, usekernel = TRUE), trControl=trainControl(verbose = TRUE, method='none'))
summary(zv)
print(zv)
print(-zv)
dim(trainData)
zv <- nearZeroVar(trainData, saveMetrics = TRUE)$zeroVar == TRUE
zv
zv <- nearZeroVar(trainData, saveMetrics = TRUE)$zeroVar == TRUE
zv
zv <- nearZeroVar(trainData, saveMetrics = TRUE)$zeroVar
zv
zv <- nearZeroVar(trainData, saveMetrics = TRUE)$zeroVar
trainIndex <- createDataPartition(tweets$sentiment,p=.90,list=FALSE)
trainData <- features_matrix[trainIndex,]
zv <- nearZeroVar(trainData, saveMetrics = TRUE)$zeroVar
zv
zv == TRUE
zv == FALSE
nearZeroVar()
?nearZeroVar
zv <- nearZeroVar(trainData, saveMetrics = TRUE)$nzv
zum(zv)
summary(zv)
trainData <- features_matrix[trainIndex, -zv]
dim(trainData)
trainData <- features_matrix[trainIndex, -zv]
summary(zv)
zv == FALSE
sum(zv == FALSE)
trainData <- features_matrix[trainIndex, zv]
testData <- features_matrix[-trainIndex, zv]
trainLabels <- tweets[trainIndex, "sentiment"]
testLabels <- tweets[-trainIndex, "sentiment"]
dim(trainData)
dim(trainData)
trainData <- trainData[zv]
dim(trainData)
trainIndex <- createDataPartition(tweets$sentiment,p=.50,list=FALSE)
trainData <- features_matrix[trainIndex,]
trainData <- features_matrix[trainIndex, -zv]
testData <- features_matrix[-trainIndex, -zv]
trainLabels <- tweets[trainIndex, "sentiment"]
testLabels <- tweets[-trainIndex, "sentiment"]
dim(trainData)
naive <- train(trainData, trainLabels, method="svmLinear", tuneGrid=data.frame(fL = 0, adjust = 0, usekernel = TRUE), trControl=trainControl(verbose = TRUE, method='none'))
naive <- train(trainData,
trainLabels,
method="svmLinear",
tuneGrid=data.frame(C = 1),
trControl=trainControl(verbose = TRUE, method='none'))
print(naive)
p <- predict(naive, testData)
summary(p)
mean(p == testLabels)
